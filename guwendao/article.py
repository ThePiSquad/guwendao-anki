"""Details of a single article"""

import re
import requests
import bs4
from bs4 import BeautifulSoup
from loguru import logger


class Article:
    """Representing a single article"""

    url: str

    title: str
    author: str
    content: str

    content_translation: list[str]
    word_translation: list[tuple[str, str]]

    translation_id: int
    appreciation_id: int
    idjm_string: str  # some identifier used for verification

    @staticmethod
    def _get_translation_id(soup: BeautifulSoup) -> int:
        """Get the translation id for current article
        Guwendao uses this id to fetch detailed content
        """
        return int(
            soup.select_one('[id^="fanyiquan"]').get("id").replace("fanyiquan", "")
        )

    @staticmethod
    def _get_appreciation_id(soup: BeautifulSoup) -> int:
        return int(
            soup.select_one('[id^="shangxiquan"]').get("id").replace("shangxiquan", "")
        )

    @staticmethod
    def _get_idjm_string(soup: BeautifulSoup) -> str:
        href = soup.find_all("a", href=re.compile(r"^javascript:shangxiShow"))[0].get(
            "href"
        )
        match = re.search(r"shangxiShow\(\d+,'([A-Z0-9]{16})'\)", href)
        if match:
            return match.group(1)
        else:
            raise ValueError(href)

    def __init__(self, url):
        self.url = url
        self.content_translation = list()
        self.word_translation = list()
        self._load_basic()
        self._load_detail()

    def _load_basic(self):
        logger.info(f"fetching article {self.url}")
        response = requests.get(self.url, timeout=1000)
        if response.status_code != 200:
            raise ConnectionRefusedError()

        logger.info("parsing content")
        soup = BeautifulSoup(response.text, "lxml")

        content_block = soup.h1.parent
        self.title = content_block.select_one("h1").text.strip()
        self.author = content_block.select_one("p").text.strip()
        self.content = (
            content_block.select_one(".contson").text.replace(" ", "").strip()
        )

        self.translation_id = self._get_translation_id(soup)
        self.appreciation_id = self._get_appreciation_id(soup)
        self.idjm_string = self._get_idjm_string(soup)
        
        self._load_detail()

        logger.info(
            f""""parsing result:
title: {self.title}
author: {self.author}
translation_id:{self.translation_id}
appreciation_id:{self.appreciation_id}
idjm_string:{self.idjm_string}
content:\n{self.content}
content_translation:\n{self.content_translation}
world_translation:\n{self.word_translation}"""
        )

    def _load_detail(self):
        self._load_translation()

    def _load_translation(self):
        # https://www.gushiwen.cn/nocdn/ajaxfanyi.aspx?id=57185&idjm=9551045DD9900B21
        logger.info("loading translation...")
        response = requests.get(
            f"https://www.gushiwen.cn/nocdn/ajaxfanyi.aspx?id={self.translation_id}&idjm={self.idjm_string}",
            timeout=1000,
        )
        if response.status_code != 200:
            raise ConnectionRefusedError()
        logger.info("parsing translation data")
        soup = BeautifulSoup(response.text, "lxml")

        content_block = soup.select_one(".contyishang")
        children = content_block.select("p")

        content_translation = children[0]
        word_translation = children[1]

        for element in content_translation.children:
            text: str = element.text
            if text.startswith("译文") or len(text) == 0:
                continue
            self.content_translation.append(text)

        self.word_translation = _parse_world_translation(word_translation)


def _parse_world_translation(tag: bs4.Tag) -> list[tuple[str, str]]:
    """Partially generated by AI(Grok)"""
    lines = []
    current = []

    # Traverse every direct child of <p>
    for elem in tag.children:
        if elem.name == "br":
            # Finish current line when hitting <br>
            if current:
                text = "".join(str(x) for x in current).strip()
                clean = BeautifulSoup(text, "lxml").get_text()
                if clean and clean != "注释" and not clean.endswith("▲"):
                    lines.append(clean)
                current = []
        else:
            # Accumulate everything else (text, <strong>, <a>, etc.)
            current.append(elem)

    # Critical: capture the last segment (after final <br> or at the very end)
    if current:
        text = "".join(str(x) for x in current).strip()
        clean = BeautifulSoup(text, "lxml").get_text()
        lines.append(clean[:-1])

    return [tuple(text.split("：", maxsplit=1)) for text in lines]
